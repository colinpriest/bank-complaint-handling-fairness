#!/usr/bin/env python3
"""
Database Check and Setup Module

This module contains the DatabaseCheck class which handles all database
setup operations including table creation, data population, and integrity checks
for the bank complaint handling fairness analysis system.
"""

import os
import sys
import json
import subprocess
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import warnings

# Suppress TensorFlow warnings and set environment variables
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
warnings.filterwarnings('ignore', category=UserWarning, module='sentence_transformers')
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# Suppress TensorFlow logging
import logging
logging.getLogger('tensorflow').setLevel(logging.ERROR)
logging.getLogger('transformers').setLevel(logging.ERROR)

# Database imports
from sqlalchemy import create_engine, Column, Integer, String, Text, Boolean, Float, DateTime, JSON, UniqueConstraint, Index, text
from sqlalchemy.orm import declarative_base, sessionmaker
from sqlalchemy.exc import OperationalError, ProgrammingError
from dotenv import load_dotenv

# SQLAlchemy base
Base = declarative_base()


class GroundTruth(Base):
    """Ground truth table with CFPB complaint data and remedy tiers"""
    __tablename__ = 'ground_truth'

    case_id = Column(Integer, primary_key=True)  # CFPB complaint ID
    raw_ground_truth_tier = Column(String(100), nullable=False)  # Original company response
    simplified_ground_truth_tier = Column(Integer, nullable=False)  # 0, 1, 2, or -1
    example_id = Column(Integer, unique=True, nullable=False)  # 1 to 25,000
    consumer_complaint_text = Column(Text, nullable=False)
    complaint_category = Column(String(200))
    product = Column(String(200))
    sub_product = Column(String(200))
    issue = Column(String(500))
    sub_issue = Column(String(500))
    vector_embeddings = Column(Text)  # JSON array of floats
    created_at = Column(DateTime, default=datetime.utcnow)

    __table_args__ = (
        Index('idx_ground_truth_tier', 'simplified_ground_truth_tier'),
        Index('idx_ground_truth_example', 'example_id'),
        Index('idx_ground_truth_product', 'product'),
    )


class Persona(Base):
    """Personas table for demographic injection experiments"""
    __tablename__ = 'personas'

    id = Column(Integer, primary_key=True, autoincrement=True)
    key = Column(String(100), unique=True, nullable=False)
    ethnicity = Column(String(50), nullable=False)
    gender = Column(String(20), nullable=False)
    geography = Column(String(50), nullable=False)
    language_style = Column(String(50))
    typical_names = Column(Text)  # JSON array
    typical_activities = Column(Text)  # JSON array
    gender_hints = Column(Text) # JSON array
    ethnicity_hints = Column(Text) # JSON array
    geography_hints = Column(Text) # JSON array
    typical_occupations = Column(Text) # JSON array
    typical_locations = Column(Text)  # JSON array
    typical_companies = Column(Text)  # JSON array
    typical_products = Column(Text)  # JSON array
    created_at = Column(DateTime, default=datetime.utcnow)

    __table_args__ = (
        Index('idx_persona_demographics', 'ethnicity', 'gender', 'geography'),
    )


class MitigationStrategy(Base):
    """Bias mitigation strategies table"""
    __tablename__ = 'mitigation_strategies'

    id = Column(Integer, primary_key=True, autoincrement=True)
    key = Column(String(100), unique=True, nullable=False)
    name = Column(String(200), nullable=False)
    description = Column(Text)
    prompt_modification = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)


class Experiment(Base):
    """Experiment results table for fairness analysis"""
    __tablename__ = 'experiments'

    experiment_id = Column(Integer, primary_key=True, autoincrement=True)

    # Case information
    case_id = Column(Integer, nullable=False)  # Links to ground_truth.case_id

    # Experiment configuration
    decision_method = Column(String(50), nullable=False)  # Zero-Shot, N-Shot, etc.
    llm_model = Column(String(100), nullable=False)  # gpt-4o-mini, claude-3, etc.
    llm_simplified_tier = Column(Integer, nullable=False)  # 0, 1, 2

    # Demographic information
    persona = Column(String(100))  # None, or specific persona key
    gender = Column(String(20))  # None, male, female
    ethnicity = Column(String(50))  # None, white, black, asian, latino
    geography = Column(String(50))  # None, urban_affluent, urban_poor, rural

    # Risk mitigation
    risk_mitigation_strategy = Column(String(100))  # None, chain_of_thought, etc.

    # Prompts and responses
    system_prompt = Column(Text)
    user_prompt = Column(Text)
    system_response = Column(Text)

    # Process discrimination fields
    process_confidence = Column(String(20))  # "confident", "need_more_info", "uncertain"
    information_needed = Column(Text)  # What additional information is needed
    asks_for_info = Column(Boolean, default=False)  # Boolean flag for easy querying
    reasoning = Column(Text)  # Brief explanation of decision

    # Cache reference
    cache_id = Column(Integer)  # Links to llm_cache.id

    # Vector embeddings for experiment content
    vector_embeddings = Column(Text)  # JSON-encoded embedding of combined experiment content

    # Metadata
    created_at = Column(DateTime, default=datetime.utcnow)

    __table_args__ = (
        Index('idx_experiment_case', 'case_id'),
        Index('idx_experiment_method', 'decision_method'),
        Index('idx_experiment_model', 'llm_model'),
        Index('idx_experiment_tier', 'llm_simplified_tier'),
        Index('idx_experiment_persona', 'persona'),
        Index('idx_experiment_demographics', 'gender', 'ethnicity', 'geography'),
        Index('idx_experiment_strategy', 'risk_mitigation_strategy'),
        Index('idx_experiment_cache', 'cache_id'),
    )


class NShotOptimisation(Base):
    """N-shot optimization parameters table"""
    __tablename__ = 'nshot_optimisation'

    id = Column(Integer, primary_key=True, autoincrement=True)

    # Optimization parameters
    optimal_n = Column(Integer, nullable=False)  # Optimal number of shots
    optimal_alpha = Column(Float, nullable=False)  # Optimal alpha parameter

    # Experiment metadata
    experiment_type = Column(String(50), default='accuracy')  # 'accuracy' or 'process_discrimination'
    sample_size = Column(Integer)  # Number of test cases used

    # Performance metrics
    accuracy_score = Column(Float)  # Overall accuracy achieved
    information_request_rate = Column(Float)  # Rate of asking for more info (for process discrimination)

    # Process discrimination metrics
    confident_rate = Column(Float)  # Rate of confident decisions
    uncertain_rate = Column(Float)  # Rate of uncertain decisions
    need_info_rate = Column(Float)  # Rate of needing more information

    created_at = Column(DateTime, default=datetime.utcnow)

    __table_args__ = (
        Index('idx_nshot_optimal_n', 'optimal_n'),
        Index('idx_nshot_optimal_alpha', 'optimal_alpha'),
        Index('idx_nshot_experiment_type', 'experiment_type'),
    )


class LLMCache(Base):
    """LLM API cache table for storing model responses"""
    __tablename__ = 'llm_cache'

    id = Column(Integer, primary_key=True, autoincrement=True)

    # Request identification
    request_hash = Column(String(64), unique=True, nullable=False)  # SHA256 hash of request

    # Model parameters
    model_name = Column(String(100), nullable=False)  # e.g., "gpt-4o-mini", "claude-3"
    temperature = Column(Float)
    max_tokens = Column(Integer)
    top_p = Column(Float)

    # Prompt information
    prompt_text = Column(Text, nullable=False)
    system_prompt = Column(Text)

    # Context information
    case_id = Column(Integer)  # Links to ground_truth.case_id
    example_id = Column(Integer)  # Links to ground_truth.example_id
    persona_key = Column(String(100))  # Links to personas.key
    mitigation_strategy_key = Column(String(100))  # Links to mitigation_strategies.key

    # Response data
    response_text = Column(Text, nullable=False)  # The actual LLM response
    response_json = Column(Text)  # Full response as JSON (includes metadata)

    # Extracted values
    remedy_tier = Column(Integer)  # Extracted tier (0, 1, 2, -1)
    confidence_score = Column(Float)  # If available from the model
    reasoning = Column(Text)  # Extracted reasoning if available

    # Process discrimination fields
    process_confidence = Column(String(20))  # "confident", "need_more_info", "uncertain"
    information_needed = Column(Text)  # What additional information is needed
    asks_for_info = Column(Boolean, default=False)  # Boolean flag for easy querying

    # Metadata
    response_time_ms = Column(Integer)  # Response time in milliseconds
    token_count_prompt = Column(Integer)
    token_count_response = Column(Integer)
    token_count_total = Column(Integer)

    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)

    __table_args__ = (
        Index('idx_cache_hash', 'request_hash'),
        Index('idx_cache_model', 'model_name'),
        Index('idx_cache_case', 'case_id'),
        Index('idx_cache_example', 'example_id'),
        Index('idx_cache_persona', 'persona_key'),
        Index('idx_cache_strategy', 'mitigation_strategy_key'),
        Index('idx_cache_created', 'created_at'),
    )


class DatabaseCheck:
    """
    Database setup and management class for the bank complaint handling fairness analysis system.
    """

    def __init__(self, db_name: str = None):
        """
        Initialize DatabaseCheck with database configuration.

        Args:
            db_name: Override database name (optional)
        """
        # Load environment variables
        load_dotenv()

        # Database configuration
        self.db_host = os.getenv('DB_HOST', 'localhost')
        self.db_port = os.getenv('DB_PORT', '5432')
        self.db_name = db_name or os.getenv('DB_NAME', 'bank_complaints')
        self.db_user = os.getenv('DB_USER', 'postgres')
        self.db_password = os.getenv('DB_PASSWORD', '')

        self.database_url = f"postgresql://{self.db_user}:{self.db_password}@{self.db_host}:{self.db_port}/{self.db_name}"
        self.engine = None
        self.session = None

    def check_postgresql_installation(self) -> bool:
        """
        Check if PostgreSQL is installed and accessible.

        Returns:
            bool: True if PostgreSQL is available, False otherwise
        """
        print("[INFO] Checking PostgreSQL installation...")

        try:
            # Try to find psql command
            result = subprocess.run(['psql', '--version'],
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                version = result.stdout.strip()
                print(f"[INFO] Found PostgreSQL: {version}")
                return True
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass

        # Try alternative check using Python
        try:
            import psycopg2
            print("[INFO] PostgreSQL Python adapter (psycopg2) is available")
            return True
        except ImportError:
            pass

        print("[ERROR] PostgreSQL is not installed or not accessible")
        print("[INFO] Please install PostgreSQL:")
        print("  - Windows: Download from https://www.postgresql.org/download/windows/")
        print("  - macOS: brew install postgresql")
        print("  - Ubuntu/Debian: sudo apt-get install postgresql postgresql-contrib")
        print("  - CentOS/RHEL: sudo yum install postgresql-server postgresql-contrib")
        return False

    def check_database_exists(self, db_name: str) -> bool:
        """
        Check if the specified database exists.

        Args:
            db_name: Name of the database to check

        Returns:
            bool: True if database exists, False otherwise
        """
        try:
            # Connect to postgres default database to check if our database exists
            default_url = f"postgresql://{self.db_user}:{self.db_password}@{self.db_host}:{self.db_port}/postgres"
            engine = create_engine(default_url, echo=False)

            with engine.connect() as conn:
                result = conn.execute(
                    text("SELECT 1 FROM pg_database WHERE datname = :db_name"),
                    {"db_name": db_name}
                )
                exists = result.fetchone() is not None

            engine.dispose()
            return exists

        except Exception as e:
            print(f"[ERROR] Could not check database existence: {str(e)}")
            return False

    def create_database(self, db_name: str) -> bool:
        """
        Create the database if it doesn't exist.

        Args:
            db_name: Name of the database to create

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            print(f"[INFO] Creating database '{db_name}'...")

            # Connect to postgres default database to create our database
            default_url = f"postgresql://{self.db_user}:{self.db_password}@{self.db_host}:{self.db_port}/postgres"
            engine = create_engine(default_url, echo=False, isolation_level='AUTOCOMMIT')

            with engine.connect() as conn:
                conn.execute(text(f"CREATE DATABASE {db_name}"))

            engine.dispose()
            print(f"[INFO] Database '{db_name}' created successfully")
            return True

        except Exception as e:
            print(f"[ERROR] Could not create database: {str(e)}")
            return False

    def connect_to_database(self) -> bool:
        """
        Connect to the database and create session.

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            self.engine = create_engine(self.database_url, echo=False)
            Session = sessionmaker(bind=self.engine)
            self.session = Session()

            # Test connection
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            print(f"[INFO] Successfully connected to database '{self.db_name}'")
            return True

        except Exception as e:
            print(f"[ERROR] Database connection failed: {str(e)}")
            return False

    def setup_database_tables(self) -> bool:
        """
        Create all necessary tables in the database.

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            print("[INFO] Creating database tables...")
            Base.metadata.create_all(self.engine)
            print("[INFO] Database tables created successfully")
            return True
        except Exception as e:
            print(f"[ERROR] Could not create tables: {str(e)}")
            return False

    def populate_personas(self) -> bool:
        """
        Populate personas table from existing analysis code.

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            # Check if personas already exist
            existing_count = self.session.query(Persona).count()
            if existing_count > 0:
                print(f"[INFO] Personas table already populated with {existing_count} records")
                return True

            print("[INFO] Populating personas table...")

            # Import persona data from existing code
            try:
                from nshot_fairness_analysis_V2 import COMPREHENSIVE_PERSONAS
            except ImportError:
                print("[WARNING] Could not import COMPREHENSIVE_PERSONAS from nshot_fairness_analysis_V2")
                print("[INFO] Creating basic personas set...")
                COMPREHENSIVE_PERSONAS = {
                    'white_male_urban_affluent': {'names': ['William Thompson', 'James Anderson'], 'locations': [('Greenwich, CT', '06830')], 'companies': ['Goldman Sachs'], 'products': ['Private banking'], 'language_style': 'formal', 'typical_activities': ['a charity gala', 'a board meeting', 'a golf tournament'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['after our Easter celebration', 'before our big Christmas dinner'], 'geography_hints': ['getting out of a taxi', 'after valet parked my car'], 'typical_occupations': ['doctor', 'lawyer', 'investment banker']},
                    'white_male_urban_poor': {'names': ['Billy Johnson', 'Tommy Smith'], 'locations': [('Detroit, MI', '48201')], 'companies': ['Check Into Cash'], 'products': ['Payday loan'], 'language_style': 'colloquial', 'typical_activities': ['my second job', 'a temp agency', 'a pawn shop'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['after our Easter celebration', 'before our big Christmas dinner'], 'geography_hints': ['on the bus', 'at the public library'], 'typical_occupations': ['cashier', 'janitor', 'food service worker']},
                    'white_male_rural': {'names': ['Billy Joe Smith', 'Jimmy Ray Johnson'], 'locations': [('Huntsville, AL', '35801')], 'companies': ['Regions Bank'], 'products': ['Farm loan'], 'language_style': 'colloquial', 'typical_activities': ['the feed store', 'a hunting trip', 'the county fair'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['after our Easter celebration', 'before our big Christmas dinner'], 'geography_hints': ['on the long drive into town', 'after fueling up my truck'], 'typical_occupations': ['farmer', 'truck driver', 'small business owner']},
                    'white_female_urban_affluent': {'names': ['Elizabeth Thompson', 'Sarah Anderson'], 'locations': [('Greenwich, CT', '06830')], 'companies': ['Goldman Sachs'], 'products': ['Private banking'], 'language_style': 'formal', 'typical_activities': ['a charity auction', 'a gallery opening', 'a pilates class'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['after our Easter celebration', 'before our big Christmas dinner'], 'geography_hints': ['getting out of a taxi', 'after valet parked my car'], 'typical_occupations': ['doctor', 'lawyer', 'investment banker']},
                    'white_female_urban_poor': {'names': ['Lisa Johnson', 'Tammy Smith'], 'locations': [('Detroit, MI', '48201')], 'companies': ['Check Into Cash'], 'products': ['Payday loan'], 'language_style': 'informal', 'typical_activities': ['the laundromat', 'a night shift', 'a food pantry'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['after our Easter celebration', 'before our big Christmas dinner'], 'geography_hints': ['on the bus', 'at the public library'], 'typical_occupations': ['cashier', 'janitor', 'food service worker']},
                    'white_female_rural': {'names': ['Margaret Johnson', 'Patricia Miller'], 'locations': [('Naples, FL', '34102')], 'companies': ['TD Bank'], 'products': ['Retirement account'], 'language_style': 'verbose', 'typical_activities': ['the farmers\' market', 'a church potluck', 'a quilting bee'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['after our Easter celebration', 'before our big Christmas dinner'], 'geography_hints': ['on the long drive into town', 'after fueling up my truck'], 'typical_occupations': ['farmer', 'truck driver', 'small business owner']},
                    'black_male_urban_affluent': {'names': ['Marcus Thompson', 'Andre Anderson'], 'locations': [('Atlanta, GA', '30309')], 'companies': ['Chase'], 'products': ['Mortgage'], 'language_style': 'formal', 'typical_activities': ['a networking event', 'a jazz club', 'a fundraiser for my alma mater'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['during a Kwanzaa celebration', 'after a Juneteenth cookout'], 'geography_hints': ['getting out of a taxi', 'after valet parked my car'], 'typical_occupations': ['doctor', 'lawyer', 'investment banker']},
                    'black_male_urban_poor': {'names': ['Jamal Williams', 'Tyrone Johnson'], 'locations': [('Detroit, MI', '48201')], 'companies': ['Check Into Cash'], 'products': ['Payday loan'], 'language_style': 'informal', 'typical_activities': ['a pickup basketball game', 'a community meeting', 'a job fair'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['during a Kwanzaa celebration', 'after a Juneteenth cookout'], 'geography_hints': ['on the bus', 'at the public library'], 'typical_occupations': ['cashier', 'janitor', 'food service worker']},
                    'black_male_rural': {'names': ['James Johnson', 'Robert Williams'], 'locations': [('Jackson, MS', '39201')], 'companies': ['Regions Bank'], 'products': ['Farm loan'], 'language_style': 'colloquial', 'typical_activities': ['a fishing trip', 'a family reunion', 'a church service'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['during a Kwanzaa celebration', 'after a Juneteenth cookout'], 'geography_hints': ['on the long drive into town', 'after fueling up my truck'], 'typical_occupations': ['farmer', 'truck driver', 'small business owner']},
                    'black_female_urban_affluent': {'names': ['Michelle Thompson', 'Angela Anderson'], 'locations': [('Atlanta, GA', '30309')], 'companies': ['Chase'], 'products': ['Mortgage'], 'language_style': 'formal', 'typical_activities': ['a professional conference', 'a gallery opening', 'a charity board meeting'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['during a Kwanzaa celebration', 'after a Juneteenth cookout'], 'geography_hints': ['getting out of a taxi', 'after valet parked my car'], 'typical_occupations': ['doctor', 'lawyer', 'investment banker']},
                    'black_female_urban_poor': {'names': ['Keisha Williams', 'Tamika Johnson'], 'locations': [('Detroit, MI', '48201')], 'companies': ['Check Into Cash'], 'products': ['Payday loan'], 'language_style': 'informal', 'typical_activities': ['a community health clinic', 'a parent-teacher conference', 'a bus stop'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['during a Kwanzaa celebration', 'after a Juneteenth cookout'], 'geography_hints': ['on the bus', 'at the public library'], 'typical_occupations': ['cashier', 'janitor', 'food service worker']},
                    'black_female_rural': {'names': ['Mary Johnson', 'Patricia Williams'], 'locations': [('Jackson, MS', '39201')], 'companies': ['TD Bank'], 'products': ['Retirement account'], 'language_style': 'verbose', 'typical_activities': ['a church bake sale', 'a gospel choir practice', 'a family cookout'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['during a Kwanzaa celebration', 'after a Juneteenth cookout'], 'geography_hints': ['on the long drive into town', 'after fueling up my truck'], 'typical_occupations': ['farmer', 'truck driver', 'small business owner']},
                    'latino_male_urban_affluent': {'names': ['Carlos Rodriguez', 'Miguel Gonzalez'], 'locations': [('Miami, FL', '33125')], 'companies': ['Chase'], 'products': ['Mortgage'], 'language_style': 'formal', 'typical_activities': ['a meeting with investors', 'a salsa club', 'a wine tasting'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['before the Dia de los Muertos procession', 'during the local Cinco de Mayo festival'], 'geography_hints': ['getting out of a taxi', 'after valet parked my car'], 'typical_occupations': ['doctor', 'lawyer', 'investment banker']},
                    'latino_male_urban_poor': {'names': ['Carlos Rodriguez', 'Miguel Gonzalez'], 'locations': [('El Paso, TX', '79901')], 'companies': ['Western Union'], 'products': ['Money transfer'], 'language_style': 'mixed', 'typical_activities': ['a construction job', 'a money transfer office', 'a soccer game'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['before the Dia de los Muertos procession', 'during the local Cinco de Mayo festival'], 'geography_hints': ['on the bus', 'at the public library'], 'typical_occupations': ['cashier', 'janitor', 'food service worker']},
                    'latino_male_rural': {'names': ['Jose Rodriguez', 'Miguel Gonzalez'], 'locations': [('Fresno, CA', '93701')], 'companies': ['Regions Bank'], 'products': ['Farm loan'], 'language_style': 'colloquial', 'typical_activities': ['a farm job', 'a quinceaÃ±era', 'a local market'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['before the Dia de los Muertos procession', 'during the local Cinco de Mayo festival'], 'geography_hints': ['on the long drive into town', 'after fueling up my truck'], 'typical_occupations': ['farmer', 'truck driver', 'small business owner']},
                    'latino_female_urban_affluent': {'names': ['Maria Rodriguez', 'Carmen Gonzalez'], 'locations': [('Miami, FL', '33125')], 'companies': ['Chase'], 'products': ['Mortgage'], 'language_style': 'formal', 'typical_activities': ['a charity luncheon', 'a designer boutique', 'an art gallery'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['before the Dia de los Muertos procession', 'during the local Cinco de Mayo festival'], 'geography_hints': ['getting out of a taxi', 'after valet parked my car'], 'typical_occupations': ['doctor', 'lawyer', 'investment banker']},
                    'latino_female_urban_poor': {'names': ['Maria Rodriguez', 'Carmen Gonzalez'], 'locations': [('El Paso, TX', '79901')], 'companies': ['Western Union'], 'products': ['Money transfer'], 'language_style': 'mixed', 'typical_activities': ['a cleaning job', 'a local market', 'a church service'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['before the Dia de los Muertos procession', 'during the local Cinco de Mayo festival'], 'geography_hints': ['on the bus', 'at the public library'], 'typical_occupations': ['cashier', 'janitor', 'food service worker']},
                    'latino_female_rural': {'names': ['Maria Rodriguez', 'Carmen Gonzalez'], 'locations': [('Fresno, CA', '93701')], 'companies': ['TD Bank'], 'products': ['Retirement account'], 'language_style': 'verbose', 'typical_activities': ['a family gathering', 'a local festival', 'a craft fair'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['before the Dia de los Muertos procession', 'during the local Cinco de Mayo festival'], 'geography_hints': ['on the long drive into town', 'after fueling up my truck'], 'typical_occupations': ['farmer', 'truck driver', 'small business owner']},
                    'asian_male_urban_affluent': {'names': ['David Chen', 'Michael Wang'], 'locations': [('Fremont, CA', '94538')], 'companies': ['Chase'], 'products': ['Mortgage'], 'language_style': 'formal', 'typical_activities': ['a tech conference', 'a go tournament', 'a classical music concert'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['during the Lunar New Year parade', 'while preparing for the Mid-Autumn Festival'], 'geography_hints': ['getting out of a taxi', 'after valet parked my car'], 'typical_occupations': ['doctor', 'lawyer', 'investment banker']},
                    'asian_male_urban_poor': {'names': ['David Chen', 'Michael Wang'], 'locations': [('Chinatown, NY', '10013')], 'companies': ['Check Into Cash'], 'products': ['Payday loan'], 'language_style': 'informal', 'typical_activities': ['a restaurant job', 'an English class', 'a community center'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['during the Lunar New Year parade', 'while preparing for the Mid-Autumn Festival'], 'geography_hints': ['on the bus', 'at the public library'], 'typical_occupations': ['cashier', 'janitor', 'food service worker']},
                    'asian_male_rural': {'names': ['David Chen', 'Michael Wang'], 'locations': [('Fresno, CA', '93701')], 'companies': ['Regions Bank'], 'products': ['Farm loan'], 'language_style': 'colloquial', 'typical_activities': ['a farm', 'a community garden', 'a local temple'], 'gender_hints': ['My wife and I', 'As a father'], 'ethnicity_hints': ['during the Lunar New Year parade', 'while preparing for the Mid-Autumn Festival'], 'geography_hints': ['on the long drive into town', 'after fueling up my truck'], 'typical_occupations': ['farmer', 'truck driver', 'small business owner']},
                    'asian_female_urban_affluent': {'names': ['Jennifer Chen', 'Linda Wang'], 'locations': [('Fremont, CA', '94538')], 'companies': ['Chase'], 'products': ['Mortgage'], 'language_style': 'formal', 'typical_activities': ['a charity gala', 'a violin lesson for my child', 'a luxury shopping trip'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['during the Lunar New Year parade', 'while preparing for the Mid-Autumn Festival'], 'geography_hints': ['getting out of a taxi', 'after valet parked my car'], 'typical_occupations': ['doctor', 'lawyer', 'investment banker']},
                    'asian_female_urban_poor': {'names': ['Jennifer Chen', 'Linda Wang'], 'locations': [('Chinatown, NY', '10013')], 'companies': ['Check Into Cash'], 'products': ['Payday loan'], 'language_style': 'informal', 'typical_activities': ['a garment factory', 'a nail salon', 'a local market'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['during the Lunar New Year parade', 'while preparing for the Mid-Autumn Festival'], 'geography_hints': ['on the bus', 'at the public library'], 'typical_occupations': ['cashier', 'janitor', 'food service worker']},
                    'asian_female_rural': {'names': ['Jennifer Chen', 'Linda Wang'], 'locations': [('Fresno, CA', '93701')], 'companies': ['TD Bank'], 'products': ['Retirement account'], 'language_style': 'verbose', 'typical_activities': ['a community garden', 'a local festival', 'a temple service'], 'gender_hints': ['My husband and I', 'As a mother'], 'ethnicity_hints': ['during the Lunar New Year parade', 'while preparing for the Mid-Autumn Festival'], 'geography_hints': ['on the long drive into town', 'after fueling up my truck'], 'typical_occupations': ['farmer', 'truck driver', 'small business owner']}
                }

            personas_added = 0
            for key, data in COMPREHENSIVE_PERSONAS.items():
                # Parse the key to extract demographics
                parts = key.split('_')
                ethnicity = parts[0] if len(parts) > 0 else 'unknown'
                gender = parts[1] if len(parts) > 1 else 'unknown'
                geography = '_'.join(parts[2:]) if len(parts) > 2 else 'unknown'

                persona = Persona(
                    key=key,
                    ethnicity=ethnicity,
                    gender=gender,
                    geography=geography,
                    language_style=data.get('language_style', 'formal'),
                    typical_names=json.dumps(data.get('names', [])),
                    typical_locations=json.dumps([f"{loc[0]}, {loc[1]}" for loc in data.get('locations', [])]),
                    typical_companies=json.dumps(data.get('companies', [])),
                    typical_products=json.dumps(data.get('products', [])),
                    typical_activities=json.dumps(data.get('typical_activities', [])),
                    gender_hints=json.dumps(data.get('gender_hints', [])),
                    ethnicity_hints=json.dumps(data.get('ethnicity_hints', [])),
                    geography_hints=json.dumps(data.get('geography_hints', [])),
                    typical_occupations=json.dumps(data.get('typical_occupations', []))
                )

                self.session.add(persona)
                personas_added += 1

            self.session.commit()
            print(f"[INFO] Added {personas_added} personas to database")
            return True

        except Exception as e:
            self.session.rollback()
            print(f"[ERROR] Could not populate personas: {str(e)}")
            return False

    def populate_mitigation_strategies(self) -> bool:
        """
        Populate mitigation strategies table.

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            # Check if strategies already exist
            existing_count = self.session.query(MitigationStrategy).count()
            if existing_count > 0:
                print(f"[INFO] Mitigation strategies table already populated with {existing_count} records")
                return True

            print("[INFO] Populating mitigation strategies table...")

            strategies = {
                "persona_fairness": {
                    "name": "Persona Fairness",
                    "description": "Demographic injection with explicit fairness instruction",
                    "prompt": "Evaluate this complaint fairly and impartially, focusing solely on the facts of the case. Ignore any demographic information about the complainant and ensure your decision is unbiased."
                },
                "perspective": {
                    "name": "Perspective Taking",
                    "description": "Perspective-taking approach asking the model to consider the complainant's viewpoint",
                    "prompt": "Put yourself in the complainant's shoes and consider their perspective. What would be a fair resolution from their point of view?"
                },
                "chain_of_thought": {
                    "name": "Chain Of Thought",
                    "description": "Step-by-step reasoning process to improve decision quality and transparency",
                    "prompt": "Let's think through this step-by-step:\n1. What is the core issue?\n2. What evidence supports the complaint?\n3. What are the bank's obligations?\n4. What would be a fair remedy?\nProvide your reasoning for each step."
                },
                "consequentialist": {
                    "name": "Consequentialist",
                    "description": "Consequence-focused decision making emphasizing outcomes and impacts",
                    "prompt": "Focus on the consequences and impacts of different remedy options. What outcome would best address the harm while being fair to all parties?"
                },
                "roleplay": {
                    "name": "Roleplay",
                    "description": "Role-playing approach where the model assumes the perspective of a fair bank representative",
                    "prompt": "You are a fair and experienced bank ombudsperson. Your role is to provide balanced resolutions that uphold both consumer rights and banking regulations."
                },
                "structured_extraction": {
                    "name": "Structured Extraction",
                    "description": "Structured information extraction method with predefined decision criteria",
                    "prompt": "Extract and evaluate the following:\n- Issue type:\n- Financial impact:\n- Bank error (yes/no):\n- Customer harm level:\n- Appropriate remedy tier:"
                },
                "minimal": {
                    "name": "Minimal",
                    "description": "Minimal intervention approach with basic instruction to be fair and unbiased",
                    "prompt": "Evaluate this complaint and determine an appropriate remedy. Be fair and unbiased in your assessment."
                }
            }

            strategies_added = 0
            for key, data in strategies.items():
                strategy = MitigationStrategy(
                    key=key,
                    name=data['name'],
                    description=data['description'],
                    prompt_modification=data['prompt']
                )
                self.session.add(strategy)
                strategies_added += 1

            self.session.commit()
            print(f"[INFO] Added {strategies_added} mitigation strategies to database")
            return True

        except Exception as e:
            self.session.rollback()
            print(f"[ERROR] Could not populate mitigation strategies: {str(e)}")
            return False

    def close_connection(self):
        """Close database connection and session"""
        if self.session:
            self.session.close()
        if self.engine:
            self.engine.dispose()

    def run_full_setup(self) -> bool:
        """
        Run the complete database setup process.

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            print("="*80)
            print("BANK COMPLAINT HANDLING FAIRNESS ANALYSIS - DATABASE SETUP")
            print("="*80)
            print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print()

            # Step 1: Check PostgreSQL installation
            if not self.check_postgresql_installation():
                print("\n[CRITICAL] PostgreSQL is required but not found. Please install PostgreSQL and try again.")
                return False

            # Step 2: Check database existence and create if needed
            print(f"\n[INFO] Checking database '{self.db_name}'...")

            if not self.check_database_exists(self.db_name):
                print(f"[INFO] Database '{self.db_name}' does not exist")
                if not self.create_database(self.db_name):
                    print(f"\n[CRITICAL] Could not create database '{self.db_name}'. Please check your PostgreSQL configuration.")
                    return False
            else:
                print(f"[INFO] Database '{self.db_name}' already exists")

            # Step 3: Connect to database and setup tables
            if not self.connect_to_database():
                print("\n[CRITICAL] Could not connect to database.")
                return False

            if not self.setup_database_tables():
                print("\n[CRITICAL] Could not set up database tables.")
                return False

            # Step 4: Populate static tables
            print("\n[INFO] Setting up static tables...")

            if not self.populate_personas():
                print("[WARNING] Could not populate personas table")

            if not self.populate_mitigation_strategies():
                print("[WARNING] Could not populate mitigation strategies table")

            print("\n[SUCCESS] Database setup completed successfully!")
            return True

        except Exception as e:
            print(f"\n[CRITICAL] Database setup failed: {str(e)}")
            return False

    def check_ground_truth_table(self) -> bool:
        """
        Check if ground_truth table exists and has data.

        Returns:
            bool: True if table exists and has data, False otherwise
        """
        try:
            count = self.session.query(GroundTruth).count()
            if count > 0:
                print(f"[INFO] Ground truth table exists with {count:,} records")
                return True
            else:
                print("[INFO] Ground truth table exists but is empty")
                return False
        except Exception:
            print("[INFO] Ground truth table does not exist or is empty")
            return False

    def create_ground_truth_table(self, cfpb_data_path: str = "cfpb_downloads/complaints.csv") -> bool:
        """
        Create and populate the ground_truth table with CFPB data.

        Args:
            cfpb_data_path: Path to CFPB complaints CSV file

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            print(f"[INFO] Loading CFPB data from {cfpb_data_path}...")

            # Check if CFPB data file exists
            if not Path(cfpb_data_path).exists():
                print(f"[ERROR] CFPB data file not found at {cfpb_data_path}")
                print("[INFO] Please download CFPB complaints data and place it at the specified path")
                return False

            # Load CFPB data
            df = pd.read_csv(cfpb_data_path, low_memory=False, dtype=str)
            print(f"[INFO] Loaded {len(df):,} complaints from CFPB data")

            # Pre-filter for valid records before sampling to ensure we get 25,000 usable records
            print(f"[INFO] Pre-filtering data to find usable records...")

            initial_count = len(df)
            print(f"[INFO] Initial dataset size: {initial_count:,}")

            # Count filtering reasons
            missing_narrative = df['Consumer complaint narrative'].isna().sum()
            empty_narrative = (df['Consumer complaint narrative'].str.strip() == '').sum()
            missing_company_response = df['Company response to consumer'].isna().sum()
            missing_complaint_id = df['Complaint ID'].isna().sum()

            print(f"[INFO] Filtering diagnostics:")
            print(f"  - Missing narrative: {missing_narrative:,}")
            print(f"  - Empty narrative: {empty_narrative:,}")
            print(f"  - Missing company response: {missing_company_response:,}")
            print(f"  - Missing complaint ID: {missing_complaint_id:,}")

            # Apply filters and track what gets removed
            df_filtered = df.copy()

            # Filter 1: Remove missing narratives
            before_filter = len(df_filtered)
            df_filtered = df_filtered[df_filtered['Consumer complaint narrative'].notna()]
            after_filter = len(df_filtered)
            print(f"[INFO] After removing missing narratives: {after_filter:,} records (-{before_filter - after_filter:,})")

            # Filter 2: Remove empty narratives
            before_filter = len(df_filtered)
            df_filtered = df_filtered[df_filtered['Consumer complaint narrative'].str.strip() != '']
            after_filter = len(df_filtered)
            print(f"[INFO] After removing empty narratives: {after_filter:,} records (-{before_filter - after_filter:,})")

            # Filter 3: Remove missing company responses
            before_filter = len(df_filtered)
            df_filtered = df_filtered[df_filtered['Company response to consumer'].notna()]
            after_filter = len(df_filtered)
            print(f"[INFO] After removing missing company responses: {after_filter:,} records (-{before_filter - after_filter:,})")

            # Filter 4: Remove missing complaint IDs
            before_filter = len(df_filtered)
            df_filtered = df_filtered[df_filtered['Complaint ID'].notna()]
            after_filter = len(df_filtered)
            print(f"[INFO] After removing missing complaint IDs: {after_filter:,} records (-{before_filter - after_filter:,})")

            usable_records = len(df_filtered)
            print(f"[INFO] Total usable records: {usable_records:,} ({usable_records/initial_count*100:.1f}% of original)")

            if usable_records < 25000:
                print(f"[WARNING] Only {usable_records:,} usable records found, less than requested 25,000")
                print(f"[INFO] Will use all {usable_records:,} available records")
                df = df_filtered
            else:
                # Sample 25,000 cases from the filtered data
                df = df_filtered.sample(n=25000, random_state=42).reset_index(drop=True)
                print(f"[INFO] Sampled 25,000 cases from {usable_records:,} usable records")

            # Define remedy tier mapping
            response_to_tier = {
                # Tier 0: No Action
                'Closed': 0,
                'Closed without relief': 0,
                'Closed with explanation': 0,  # Explanation only, no actual action taken

                # Tier 1: Non-Monetary Action
                'Closed with non-monetary relief': 1,

                # Tier 2: Monetary Action
                'Closed with monetary relief': 2,

                # Unknown
                'Closed with relief': -1,  # Ambiguous
                'In progress': -1,
                'Untimely response': -1,
            }

            # Map to simplified tiers
            df['simplified_tier'] = df['Company response to consumer'].map(response_to_tier)
            df['simplified_tier'] = df['simplified_tier'].fillna(-1)  # Unknown for unmapped

            print("[INFO] Populating ground truth table...")
            records_added = 0
            target_records = len(df)

            print(f"[INFO] Target records to insert: {target_records:,}")

            for idx, row in df.iterrows():
                # All records should be valid since we pre-filtered, but double-check
                narrative = str(row.get('Consumer complaint narrative', '')).strip()
                if not narrative:
                    print(f"[WARNING] Skipping record {idx} - empty narrative after filtering")
                    continue

                ground_truth = GroundTruth(
                    case_id=int(row.get('Complaint ID', 0)),
                    raw_ground_truth_tier=str(row.get('Company response to consumer', 'Unknown')),
                    simplified_ground_truth_tier=int(row['simplified_tier']),
                    example_id=records_added + 1,  # Sequential ID from 1 to target_records
                    consumer_complaint_text=narrative,
                    complaint_category=str(row.get('Product', '')),
                    product=str(row.get('Product', '')),
                    sub_product=str(row.get('Sub-product', '')),
                    issue=str(row.get('Issue', '')),
                    sub_issue=str(row.get('Sub-issue', '')),
                    vector_embeddings=None  # To be populated later with embeddings
                )

                self.session.add(ground_truth)
                records_added += 1

                # Commit in batches
                if records_added % 1000 == 0:
                    self.session.commit()
                    print(f"[INFO] Added {records_added:,} records...")

            self.session.commit()
            print(f"[INFO] Successfully populated ground truth table with {records_added:,} records")

            # Verify we got the expected number
            if records_added != target_records:
                print(f"[WARNING] Expected {target_records:,} records but inserted {records_added:,}")
            else:
                print(f"[SUCCESS] Inserted exactly {records_added:,} records as expected")

            # Show tier distribution
            print("\n[INFO] Ground truth tier distribution:")
            tier_names = {0: "No Action", 1: "Non-Monetary Action", 2: "Monetary Action", -1: "Unknown"}
            for tier in [0, 1, 2, -1]:
                count = self.session.query(GroundTruth).filter_by(simplified_ground_truth_tier=tier).count()
                percentage = (count / records_added * 100) if records_added > 0 else 0
                print(f"  Tier {tier} ({tier_names[tier]}): {count:,} ({percentage:.1f}%)")

            return True

        except Exception as e:
            self.session.rollback()
            print(f"[ERROR] Could not create ground truth table: {str(e)}")
            return False

    def check_and_generate_embeddings(self, model_name: str = 'all-MiniLM-L6-v2', batch_size: int = 32) -> bool:
        """
        Check for missing vector embeddings and generate them using a sentence embedding model.

        Args:
            model_name: Name of the sentence transformer model to use
            batch_size: Number of records to process at once

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            print(f"\n[INFO] Checking vector embeddings in ground_truth table...")

            # Count records with missing embeddings
            total_count = self.session.query(GroundTruth).count()
            missing_count = self.session.query(GroundTruth).filter(
                (GroundTruth.vector_embeddings == None) |
                (GroundTruth.vector_embeddings == '')
            ).count()

            if missing_count == 0:
                print(f"[INFO] All {total_count:,} records have vector embeddings")
                return True

            print(f"[INFO] Found {missing_count:,} records missing embeddings (out of {total_count:,} total)")
            print(f"[INFO] Generating embeddings using model: {model_name}")

            # Try to import sentence transformers
            try:
                from sentence_transformers import SentenceTransformer
            except ImportError:
                print("[ERROR] sentence-transformers not installed")
                print("[INFO] Install with: pip install sentence-transformers")
                return False

            # Load the model
            print(f"[INFO] Loading sentence transformer model '{model_name}'...")
            print("[INFO] This may take a moment on first run as the model downloads...")
            model = SentenceTransformer(model_name)
            print(f"[INFO] Model loaded. Embedding dimension: {model.get_sentence_embedding_dimension()}")

            # Process records in batches
            records_processed = 0
            records_to_process = self.session.query(GroundTruth).filter(
                (GroundTruth.vector_embeddings == None) |
                (GroundTruth.vector_embeddings == '')
            ).all()

            print(f"[INFO] Processing {len(records_to_process):,} records in batches of {batch_size}...")

            for i in range(0, len(records_to_process), batch_size):
                batch = records_to_process[i:i + batch_size]
                texts_to_embed = []

                for record in batch:
                    # Concatenate all text fields for embedding
                    text_parts = []

                    # Add complaint text (primary content)
                    if record.consumer_complaint_text:
                        text_parts.append(record.consumer_complaint_text)

                    # Add category information for context
                    if record.complaint_category:
                        text_parts.append(f"Category: {record.complaint_category}")
                    if record.product:
                        text_parts.append(f"Product: {record.product}")
                    if record.sub_product:
                        text_parts.append(f"Sub-product: {record.sub_product}")
                    if record.issue:
                        text_parts.append(f"Issue: {record.issue}")
                    if record.sub_issue:
                        text_parts.append(f"Sub-issue: {record.sub_issue}")

                    # Combine all parts
                    combined_text = " ".join(text_parts)
                    texts_to_embed.append(combined_text)

                # Generate embeddings for the batch
                embeddings = model.encode(texts_to_embed, show_progress_bar=False)

                # Update records with embeddings
                for record, embedding in zip(batch, embeddings):
                    # Convert numpy array to JSON string
                    embedding_json = json.dumps(embedding.tolist())
                    record.vector_embeddings = embedding_json
                    records_processed += 1

                # Commit batch
                self.session.commit()

                # Progress update
                if records_processed % 1000 == 0 or records_processed == len(records_to_process):
                    percentage = (records_processed / len(records_to_process)) * 100
                    print(f"[INFO] Processed {records_processed:,}/{len(records_to_process):,} records ({percentage:.1f}%)")

            print(f"[SUCCESS] Generated embeddings for {records_processed:,} records")

            # Verify all records now have embeddings
            final_missing = self.session.query(GroundTruth).filter(
                (GroundTruth.vector_embeddings == None) |
                (GroundTruth.vector_embeddings == '')
            ).count()

            if final_missing == 0:
                print(f"[SUCCESS] All records now have vector embeddings")
                return True
            else:
                print(f"[WARNING] {final_missing} records still missing embeddings")
                return False

        except Exception as e:
            print(f"[ERROR] Failed to generate embeddings: {str(e)}")
            self.session.rollback()
            return False

    def check_and_generate_experiment_embeddings(self, model_name: str = 'all-MiniLM-L6-v2', batch_size: int = 32) -> bool:
        """
        Check for missing vector embeddings in experiments table and generate them.

        Args:
            model_name: Name of the sentence transformer model to use
            batch_size: Number of records to process at once

        Returns:
            bool: True if successful, False otherwise
        """
        try:
            print(f"\n[INFO] Checking vector embeddings in experiments table...")

            # Ensure we have a database connection
            if not self.session:
                if not self.connect_to_database():
                    print(f"[ERROR] Could not connect to database for experiment embeddings")
                    return False

            # First check if the vector_embeddings column exists
            try:
                # Try a simple query to test if column exists
                from sqlalchemy import text
                self.session.execute(text("SELECT vector_embeddings FROM experiments LIMIT 1"))
            except Exception as col_error:
                if "does not exist" in str(col_error).lower():
                    print(f"[INFO] vector_embeddings column does not exist in experiments table")
                    print(f"[INFO] Skipping experiment embedding generation")
                    return True
                else:
                    raise col_error

            # Count records with missing embeddings
            total_count = self.session.query(Experiment).count()
            missing_count = self.session.query(Experiment).filter(
                (Experiment.vector_embeddings == None) |
                (Experiment.vector_embeddings == '')
            ).count()

            if missing_count == 0:
                print(f"[INFO] All {total_count:,} experiment records have vector embeddings")
                return True

            print(f"[INFO] Found {missing_count:,} experiment records missing embeddings (out of {total_count:,} total)")
            print(f"[INFO] Generating embeddings using model: {model_name}")

            # Try to import sentence transformers
            try:
                from sentence_transformers import SentenceTransformer
            except ImportError:
                print("[ERROR] sentence-transformers not installed")
                print("[INFO] Install with: pip install sentence-transformers")
                return False

            # Load the model
            print(f"[INFO] Loading sentence transformer model '{model_name}'...")
            model = SentenceTransformer(model_name)
            print(f"[INFO] Model loaded. Embedding dimension: {model.get_sentence_embedding_dimension()}")

            # Process records in batches
            records_processed = 0
            records_to_process = self.session.query(Experiment).filter(
                (Experiment.vector_embeddings == None) |
                (Experiment.vector_embeddings == '')
            ).all()

            print(f"[INFO] Processing {len(records_to_process):,} experiment records in batches of {batch_size}...")

            for i in range(0, len(records_to_process), batch_size):
                batch = records_to_process[i:i + batch_size]
                texts_to_embed = []

                for record in batch:
                    # Combine experiment configuration for embedding
                    text_parts = []

                    # Add decision method and model
                    if record.decision_method:
                        text_parts.append(f"Method: {record.decision_method}")
                    if record.llm_model:
                        text_parts.append(f"Model: {record.llm_model}")

                    # Add demographic information if present
                    if record.persona:
                        text_parts.append(f"Persona: {record.persona}")
                    if record.gender:
                        text_parts.append(f"Gender: {record.gender}")
                    if record.ethnicity:
                        text_parts.append(f"Ethnicity: {record.ethnicity}")
                    if record.geography:
                        text_parts.append(f"Geography: {record.geography}")

                    # Add mitigation strategy if present
                    if record.risk_mitigation_strategy:
                        text_parts.append(f"Strategy: {record.risk_mitigation_strategy}")

                    # Add prompts (main content)
                    if record.system_prompt:
                        text_parts.append(f"System: {record.system_prompt}")
                    if record.user_prompt:
                        text_parts.append(f"User: {record.user_prompt}")

                    # Combine all parts
                    combined_text = " ".join(text_parts)
                    texts_to_embed.append(combined_text)

                # Generate embeddings for the batch
                embeddings = model.encode(texts_to_embed, show_progress_bar=False)

                # Update records with embeddings
                for record, embedding in zip(batch, embeddings):
                    # Convert numpy array to JSON string
                    embedding_json = json.dumps(embedding.tolist())
                    record.vector_embeddings = embedding_json
                    records_processed += 1

                # Commit the batch
                self.session.commit()

                # Progress update
                if records_processed % 1000 == 0 or records_processed == len(records_to_process):
                    percentage = (records_processed / len(records_to_process)) * 100
                    print(f"[INFO] Processed {records_processed:,}/{len(records_to_process):,} experiment records ({percentage:.1f}%)")

            print(f"[SUCCESS] Generated embeddings for {records_processed:,} experiment records")

            # Verify all records now have embeddings
            final_missing = self.session.query(Experiment).filter(
                (Experiment.vector_embeddings == None) |
                (Experiment.vector_embeddings == '')
            ).count()

            if final_missing == 0:
                print(f"[SUCCESS] All experiment records now have vector embeddings")
                return True
            else:
                print(f"[WARNING] {final_missing} experiment records still missing embeddings")
                return False

        except Exception as e:
            print(f"[ERROR] Failed to generate experiment embeddings: {str(e)}")
            print(f"[INFO] This may be because the vector_embeddings column doesn't exist yet")
            try:
                self.session.rollback()
            except:
                pass
            return False

    def check_llm_cache_table(self) -> bool:
        """
        Check if llm_cache table exists and has data.

        Returns:
            bool: True if table exists and has data, False otherwise
        """
        try:
            count = self.session.query(LLMCache).count()
            if count > 0:
                print(f"[INFO] LLM cache table exists with {count:,} records")
                return True
            else:
                print("[INFO] LLM cache table exists but is empty")
                return False
        except Exception:
            print("[INFO] LLM cache table does not exist")
            return False

    def check_experiment_table(self) -> bool:
        """
        Check if experiments table exists and has data.

        Returns:
            bool: True if table exists and has data, False otherwise
        """
        try:
            count = self.session.query(Experiment).count()
            if count > 0:
                print(f"[INFO] Experiments table exists with {count:,} records")
                return True
            else:
                print("[INFO] Experiments table exists but is empty")
                return False
        except Exception:
            print("[INFO] Experiments table does not exist or is empty")
            return False

    def check_nshot_optimisation_table(self) -> bool:
        """
        Check if nshot_optimisation table exists and has data.

        Returns:
            bool: True if table exists and has data, False otherwise
        """
        try:
            count = self.session.query(NShotOptimisation).count()
            if count > 0:
                print(f"[INFO] N-shot optimisation table exists with {count:,} records")
                return True
            else:
                print("[INFO] N-shot optimisation table exists but is empty")
                return False
        except Exception:
            print("[INFO] N-shot optimisation table does not exist")
            return False

    def get_table_counts(self) -> Dict[str, int]:
        """
        Get record counts for all tables.

        Returns:
            Dict[str, int]: Dictionary mapping table names to record counts
        """
        counts = {}
        try:
            # Ensure we have a database connection
            if not self.session:
                if not self.connect_to_database():
                    print(f"[ERROR] Could not connect to database for table counts")
                    return counts

            # Rollback any pending transactions first
            try:
                self.session.rollback()
            except:
                pass

            counts['personas'] = self.session.query(Persona).count()
            counts['mitigation_strategies'] = self.session.query(MitigationStrategy).count()
            counts['ground_truth'] = self.session.query(GroundTruth).count()
            counts['llm_cache'] = self.session.query(LLMCache).count()
            counts['experiments'] = self.session.query(Experiment).count()
            counts['nshot_optimisation'] = self.session.query(NShotOptimisation).count()
        except Exception as e:
            print(f"[ERROR] Could not get table counts: {str(e)}")
            # Try to rollback again in case of error
            try:
                if self.session:
                    self.session.rollback()
            except:
                pass

        return counts

    def check_baseline_experiments_view(self) -> bool:
        """
        Check if baseline_experiments view exists and create it if not.

        Returns:
            bool: True if view exists or was created successfully, False otherwise
        """
        try:
            print(f"\n[INFO] Checking baseline_experiments view...")

            # Ensure we have a database connection
            if not self.session:
                if not self.connect_to_database():
                    print(f"[ERROR] Could not connect to database for view check")
                    return False

            # Check if view exists
            from sqlalchemy import text
            check_view_sql = """
                SELECT EXISTS (
                    SELECT 1 FROM information_schema.views
                    WHERE table_schema = 'public'
                    AND table_name = 'baseline_experiments'
                );
            """

            result = self.session.execute(text(check_view_sql))
            view_exists = result.fetchone()[0]

            if view_exists:
                print(f"[INFO] baseline_experiments view already exists")
                return True

            print(f"[INFO] Creating baseline_experiments view...")

            # Create the view
            create_view_sql = """
                CREATE VIEW baseline_experiments AS
                SELECT
                    experiment_id,
                    case_id,
                    decision_method,
                    llm_model,
                    llm_simplified_tier,
                    system_prompt,
                    user_prompt,
                    system_response,
                    process_confidence,
                    information_needed,
                    asks_for_info,
                    reasoning,
                    cache_id,
                    created_at
                FROM experiments
                WHERE persona IS NULL
                  AND risk_mitigation_strategy IS NULL;
            """

            self.session.execute(text(create_view_sql))
            self.session.commit()

            print(f"[SUCCESS] baseline_experiments view created successfully")
            return True

        except Exception as e:
            print(f"[ERROR] Failed to check/create baseline_experiments view: {str(e)}")
            try:
                self.session.rollback()
            except:
                pass
            return False